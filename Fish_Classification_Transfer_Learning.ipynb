{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2170465,"sourceType":"datasetVersion","datasetId":1165452}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## IMPORTS","metadata":{"id":"5VZC6i6u5K7A"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:01:46.408736Z","iopub.execute_input":"2025-05-07T18:01:46.408987Z","iopub.status.idle":"2025-05-07T18:02:07.278713Z","shell.execute_reply.started":"2025-05-07T18:01:46.408965Z","shell.execute_reply":"2025-05-07T18:02:07.277605Z"}},"outputs":[{"name":"stderr","text":"2025-05-07 18:01:51.959526: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746640912.247488      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746640912.324003      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"image_dir = Path('../input/a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kx0cyTPztf5e","outputId":"c21dbe6a-614f-4922-bb4e-ba4b87c0c905","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:02:07.280605Z","iopub.execute_input":"2025-05-07T18:02:07.281400Z","iopub.status.idle":"2025-05-07T18:02:07.285418Z","shell.execute_reply.started":"2025-05-07T18:02:07.281374Z","shell.execute_reply":"2025-05-07T18:02:07.284525Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Creating DF","metadata":{}},{"cell_type":"code","source":"# Get filepaths and labels\nfilepaths = list(image_dir.glob(r'**/*.png'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\n# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)\n\n# Drop GT images\nimage_df['Label'] = image_df['Label'].apply(lambda x: np.NaN if x[-2:] == 'GT' else x)\nimage_df = image_df.dropna(axis=0)\n\n# Sample 200 images from each class\nsamples = []\n\nfor category in image_df['Label'].unique():\n    category_slice = image_df.query(\"Label == @category\")\n    samples.append(category_slice.sample(200, random_state=1))\n\nimage_df = pd.concat(samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tq95AMexEx31","outputId":"d46bd484-b40c-4b08-ef99-863b56570c73","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:02:07.286580Z","iopub.execute_input":"2025-05-07T18:02:07.286904Z","iopub.status.idle":"2025-05-07T18:03:29.065901Z","shell.execute_reply.started":"2025-05-07T18:02:07.286874Z","shell.execute_reply":"2025-05-07T18:03:29.064871Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"image_df","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rcoUyiiSFQuo","outputId":"cfb2f2e9-f200-4167-c2ca-c627608d0264","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:03:29.067066Z","iopub.execute_input":"2025-05-07T18:03:29.067387Z","iopub.status.idle":"2025-05-07T18:03:29.095607Z","shell.execute_reply.started":"2025-05-07T18:03:29.067361Z","shell.execute_reply":"2025-05-07T18:03:29.094578Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                               Filepath               Label\n0     ../input/a-large-scale-fish-dataset/Fish_Datas...     Gilt-Head Bream\n1     ../input/a-large-scale-fish-dataset/Fish_Datas...            Sea Bass\n2     ../input/a-large-scale-fish-dataset/Fish_Datas...          Red Mullet\n3     ../input/a-large-scale-fish-dataset/Fish_Datas...              Shrimp\n4     ../input/a-large-scale-fish-dataset/Fish_Datas...               Trout\n...                                                 ...                 ...\n1795  ../input/a-large-scale-fish-dataset/Fish_Datas...               Trout\n1796  ../input/a-large-scale-fish-dataset/Fish_Datas...       Red Sea Bream\n1797  ../input/a-large-scale-fish-dataset/Fish_Datas...  Striped Red Mullet\n1798  ../input/a-large-scale-fish-dataset/Fish_Datas...     Black Sea Sprat\n1799  ../input/a-large-scale-fish-dataset/Fish_Datas...  Striped Red Mullet\n\n[1800 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filepath</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>../input/a-large-scale-fish-dataset/Fish_Datas...</td>\n      <td>Gilt-Head Bream</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>../input/a-large-scale-fish-dataset/Fish_Datas...</td>\n      <td>Sea Bass</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>../input/a-large-scale-fish-dataset/Fish_Datas...</td>\n      <td>Red Mullet</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>../input/a-large-scale-fish-dataset/Fish_Datas...</td>\n      <td>Shrimp</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>../input/a-large-scale-fish-dataset/Fish_Datas...</td>\n      <td>Trout</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1795</th>\n      <td>../input/a-large-scale-fish-dataset/Fish_Datas...</td>\n      <td>Trout</td>\n    </tr>\n    <tr>\n      <th>1796</th>\n      <td>../input/a-large-scale-fish-dataset/Fish_Datas...</td>\n      <td>Red Sea Bream</td>\n    </tr>\n    <tr>\n      <th>1797</th>\n      <td>../input/a-large-scale-fish-dataset/Fish_Datas...</td>\n      <td>Striped Red Mullet</td>\n    </tr>\n    <tr>\n      <th>1798</th>\n      <td>../input/a-large-scale-fish-dataset/Fish_Datas...</td>\n      <td>Black Sea Sprat</td>\n    </tr>\n    <tr>\n      <th>1799</th>\n      <td>../input/a-large-scale-fish-dataset/Fish_Datas...</td>\n      <td>Striped Red Mullet</td>\n    </tr>\n  </tbody>\n</table>\n<p>1800 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_df, test_df = train_test_split(image_df, train_size=0.7, shuffle=True, random_state=1)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-Y06XbQFb8M","outputId":"7adec15e-8636-4c30-f09c-7618dc59fa80","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:03:29.097866Z","iopub.execute_input":"2025-05-07T18:03:29.098218Z","iopub.status.idle":"2025-05-07T18:03:29.106835Z","shell.execute_reply.started":"2025-05-07T18:03:29.098191Z","shell.execute_reply":"2025-05-07T18:03:29.105549Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Loading Images","metadata":{}},{"cell_type":"code","source":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n    validation_split=0.2\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:03:29.107941Z","iopub.execute_input":"2025-05-07T18:03:29.108269Z","iopub.status.idle":"2025-05-07T18:03:29.228139Z","shell.execute_reply.started":"2025-05-07T18:03:29.108243Z","shell.execute_reply":"2025-05-07T18:03:29.227030Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='training'\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='validation'\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:03:29.229347Z","iopub.execute_input":"2025-05-07T18:03:29.230787Z","iopub.status.idle":"2025-05-07T18:03:30.561339Z","shell.execute_reply.started":"2025-05-07T18:03:29.230649Z","shell.execute_reply":"2025-05-07T18:03:30.559528Z"}},"outputs":[{"name":"stdout","text":"Found 1008 validated image filenames belonging to 9 classes.\nFound 252 validated image filenames belonging to 9 classes.\nFound 540 validated image filenames belonging to 9 classes.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Load Pre Trained Model","metadata":{}},{"cell_type":"code","source":"pretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\n\npretrained_model.trainable = False","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"2U3HO0MQPVqt","outputId":"3d87b0ed-d800-4257-c82f-6ca0be3b47aa","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:03:30.562768Z","iopub.execute_input":"2025-05-07T18:03:30.563176Z","iopub.status.idle":"2025-05-07T18:03:32.498570Z","shell.execute_reply.started":"2025-05-07T18:03:30.563141Z","shell.execute_reply":"2025-05-07T18:03:32.497482Z"}},"outputs":[{"name":"stderr","text":"2025-05-07 18:03:30.577013: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(9, activation='softmax')(x)\n\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-T0FJI8Pe23","outputId":"b1765f40-5037-4cad-c107-7465b1d30245","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:03:32.499570Z","iopub.execute_input":"2025-05-07T18:03:32.499792Z","iopub.status.idle":"2025-05-07T18:09:08.084659Z","shell.execute_reply.started":"2025-05-07T18:03:32.499775Z","shell.execute_reply":"2025-05-07T18:09:08.083295Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1s/step - accuracy: 0.6010 - loss: 1.3003 - val_accuracy: 0.9762 - val_loss: 0.1391\nEpoch 2/100\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9863 - loss: 0.0712 - val_accuracy: 0.9921 - val_loss: 0.0628\nEpoch 3/100\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9970 - loss: 0.0259 - val_accuracy: 0.9643 - val_loss: 0.1030\nEpoch 4/100\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9961 - loss: 0.0135 - val_accuracy: 0.9921 - val_loss: 0.0484\nEpoch 5/100\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.9802 - val_loss: 0.0474\nEpoch 6/100\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 0.9802 - val_loss: 0.0514\nEpoch 7/100\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9802 - val_loss: 0.0509\nEpoch 8/100\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9802 - val_loss: 0.0495\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"results = model.evaluate(test_images, verbose=0)\n\nprint(\"    Test Loss: {:.5f}\".format(results[0]))\nprint(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))","metadata":{"id":"kdob2me8Pk75","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:09:08.085967Z","iopub.execute_input":"2025-05-07T18:09:08.086429Z","iopub.status.idle":"2025-05-07T18:09:27.818825Z","shell.execute_reply.started":"2025-05-07T18:09:08.086402Z","shell.execute_reply":"2025-05-07T18:09:27.817779Z"}},"outputs":[{"name":"stdout","text":"    Test Loss: 0.02651\nTest Accuracy: 99.44%\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## All Models","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n# Define your image directory\nimage_dir = Path('../input/a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset') # <--- change this to your dataset path\n\n# Get filepaths and labels\nfilepaths = list(image_dir.glob(r'**/*.png'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\n# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)\n\n# Drop GT images\nimage_df['Label'] = image_df['Label'].apply(lambda x: np.NaN if x[-2:] == 'GT' else x)\nimage_df = image_df.dropna(axis=0)\n\n# ✅ Sample up to 200 images per class — skip classes with fewer images\nsamples = []\nmin_required = 200  # number of samples per class\n\nfor category in image_df['Label'].unique():\n    category_slice = image_df.query(\"Label == @category\")\n    if len(category_slice) >= min_required:\n        samples.append(category_slice.sample(min_required, random_state=1))\n    else:\n        print(f\"Skipping class '{category}' — only {len(category_slice)} images available.\")\n\n# ✅ Check if we have valid data\nif not samples:\n    raise ValueError(\"❌ No categories had enough images to include in the dataset.\")\n\n# ✅ Final dataset\nimage_df = pd.concat(samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:37:49.127714Z","iopub.execute_input":"2025-05-07T18:37:49.128074Z","iopub.status.idle":"2025-05-07T18:38:26.511541Z","shell.execute_reply.started":"2025-05-07T18:37:49.128050Z","shell.execute_reply":"2025-05-07T18:38:26.510346Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport os\nfrom tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2, InceptionV3, EfficientNetB0\n\n# Step 1: Split dataset into train and validation\ntrain_df, val_df = train_test_split(image_df, test_size=0.2, random_state=42)\n\n# Step 2: Set up ImageDataGenerator with preprocessing for each model\ntrain_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n)\n\nval_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n)\n\n# Load images and labels for training and validation\ntrain_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n)\n\nval_images = val_generator.flow_from_dataframe(\n    dataframe=val_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n)\n\n# Step 3: Define a function to create the models\ndef create_model(base_model):\n    model = tf.keras.Model(inputs=base_model.input, outputs=base_model.output)\n    x = tf.keras.layers.Dense(128, activation='relu')(model.output)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    outputs = tf.keras.layers.Dense(len(train_images.class_indices), activation='softmax')(x)\n    final_model = tf.keras.Model(inputs=model.input, outputs=outputs)\n    \n    final_model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return final_model\n\n# Step 4: Define models to test (VGG16, ResNet50, MobileNetV2, InceptionV3, EfficientNetB0)\nmodels = {\n    \"VGG16\": tf.keras.applications.VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet', pooling='avg'),\n    \"ResNet50\": tf.keras.applications.ResNet50(input_shape=(224, 224, 3), include_top=False, weights='imagenet', pooling='avg'),\n    \"MobileNetV2\": tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet', pooling='avg'),\n    \"InceptionV3\": tf.keras.applications.InceptionV3(input_shape=(224, 224, 3), include_top=False, weights='imagenet', pooling='avg'),\n    \"EfficientNetB0\": tf.keras.applications.EfficientNetB0(input_shape=(224, 224, 3), include_top=False, weights='imagenet', pooling='avg')\n}\n\n# Step 5: Train and evaluate each model\nresults = {}\n\nfor model_name, base_model in models.items():\n    print(f\"\\nTraining {model_name}...\")\n    \n    # Freeze base model layers\n    base_model.trainable = False\n\n    # Create model and train\n    model = create_model(base_model)\n    \n    # Train the model\n    history = model.fit(\n        train_images,\n        validation_data=val_images,\n        epochs=100,\n        callbacks=[\n            tf.keras.callbacks.EarlyStopping(\n                monitor='val_loss',\n                patience=3,\n                restore_best_weights=True\n            )\n        ]\n    )\n    \n    # Evaluate model on validation data\n    val_loss, val_accuracy = model.evaluate(val_images)\n    results[model_name] = val_accuracy\n    print(f\"{model_name} - Validation Accuracy: {val_accuracy}\")\n\n# Step 6: Print the results\nprint(\"\\nResults for all models:\")\nfor model_name, accuracy in results.items():\n    print(f\"{model_name}: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T18:39:17.235564Z","iopub.execute_input":"2025-05-07T18:39:17.235953Z","execution_failed":"2025-05-08T16:29:35.248Z"}},"outputs":[{"name":"stdout","text":"Found 1440 validated image filenames belonging to 9 classes.\nFound 360 validated image filenames belonging to 9 classes.\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n\nTraining VGG16...\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 12s/step - accuracy: 0.2424 - loss: 2.1123 - val_accuracy: 0.5917 - val_loss: 1.6224\nEpoch 2/100\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 12s/step - accuracy: 0.6193 - loss: 1.4513 - val_accuracy: 0.7806 - val_loss: 0.9465\nEpoch 3/100\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 13s/step - accuracy: 0.8067 - loss: 0.8253 - val_accuracy: 0.7944 - val_loss: 0.6666\nEpoch 4/100\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 12s/step - accuracy: 0.8688 - loss: 0.5173 - val_accuracy: 0.9139 - val_loss: 0.4213\nEpoch 5/100\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 12s/step - accuracy: 0.9220 - loss: 0.3503 - val_accuracy: 0.9333 - val_loss: 0.3454\nEpoch 6/100\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m533s\u001b[0m 12s/step - accuracy: 0.9221 - loss: 0.2659 - val_accuracy: 0.9306 - val_loss: 0.2632\nEpoch 7/100\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 13s/step - accuracy: 0.9563 - loss: 0.1977 - val_accuracy: 0.9500 - val_loss: 0.1987\nEpoch 8/100\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 12s/step - accuracy: 0.9791 - loss: 0.1555 - val_accuracy: 0.9556 - val_loss: 0.1681\nEpoch 9/100\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 0.9808 - loss: 0.1162","output_type":"stream"}],"execution_count":null}]}