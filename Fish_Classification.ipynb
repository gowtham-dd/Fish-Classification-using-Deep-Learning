{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "name": "Fish Classification",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2170465,
          "sourceType": "datasetVersion",
          "datasetId": 1165452
        }
      ],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gowtham-dd/Fish-Classification-using-Deep-Learning/blob/main/Fish_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "crowww_a_large_scale_fish_dataset_path = kagglehub.dataset_download('crowww/a-large-scale-fish-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "zwgC2OO1YHcU"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTS"
      ],
      "metadata": {
        "id": "5VZC6i6u5K7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os.path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T06:19:55.436426Z",
          "iopub.execute_input": "2025-05-09T06:19:55.436655Z",
          "iopub.status.idle": "2025-05-09T06:20:14.015468Z",
          "shell.execute_reply.started": "2025-05-09T06:19:55.436636Z",
          "shell.execute_reply": "2025-05-09T06:20:14.014784Z"
        },
        "id": "9FdMMnFMYHcx",
        "outputId": "1c73e746-12c2-49c0-d31b-db79931fbe4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-05-09 06:20:00.660853: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746771600.902651      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746771600.972649      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = Path('../input/a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset')"
      ],
      "metadata": {
        "id": "Kx0cyTPztf5e",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating DF"
      ],
      "metadata": {
        "id": "fa4wYMqhYHc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get filepaths and labels\n",
        "filepaths = list(image_dir.glob(r'**/*.png'))\n",
        "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
        "\n",
        "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
        "labels = pd.Series(labels, name='Label')\n",
        "\n",
        "# Concatenate filepaths and labels\n",
        "image_df = pd.concat([filepaths, labels], axis=1)\n",
        "\n",
        "# Drop GT images\n",
        "image_df['Label'] = image_df['Label'].apply(lambda x: np.NaN if x[-2:] == 'GT' else x)\n",
        "image_df = image_df.dropna(axis=0)\n",
        "\n",
        "# Sample 200 images from each class\n",
        "samples = []\n",
        "\n",
        "for category in image_df['Label'].unique():\n",
        "    category_slice = image_df.query(\"Label == @category\")\n",
        "    samples.append(category_slice.sample(200, random_state=1))\n",
        "\n",
        "image_df = pd.concat(samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "tq95AMexEx31",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "image_df"
      ],
      "metadata": {
        "id": "rcoUyiiSFQuo",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(image_df, train_size=0.7, shuffle=True, random_state=1)"
      ],
      "metadata": {
        "id": "i-Y06XbQFb8M",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Images"
      ],
      "metadata": {
        "id": "2aF9bTKOYHc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "rN5lcds-YHdA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_images = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "test_images = test_generator.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "N_J5t-ELYHdB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Pre Trained Model"
      ],
      "metadata": {
        "id": "Qk1-ZVPaYHdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "pretrained_model.trainable = False"
      ],
      "metadata": {
        "id": "2U3HO0MQPVqt",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "0X-Z8fn3YHdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = pretrained_model.input\n",
        "\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(9, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_images,\n",
        "    validation_data=val_images,\n",
        "    epochs=100,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "m-T0FJI8Pe23",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_images, verbose=0)\n",
        "\n",
        "print(\"    Test Loss: {:.5f}\".format(results[0]))\n",
        "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
      ],
      "metadata": {
        "id": "kdob2me8Pk75",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All Models"
      ],
      "metadata": {
        "id": "JtwAQep8YHdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Define your image directory\n",
        "image_dir = Path('../input/a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset') # <--- change this to your dataset path\n",
        "\n",
        "# Get filepaths and labels\n",
        "filepaths = list(image_dir.glob(r'**/*.png'))\n",
        "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
        "\n",
        "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
        "labels = pd.Series(labels, name='Label')\n",
        "\n",
        "# Concatenate filepaths and labels\n",
        "image_df = pd.concat([filepaths, labels], axis=1)\n",
        "\n",
        "# Drop GT images\n",
        "image_df['Label'] = image_df['Label'].apply(lambda x: np.NaN if x[-2:] == 'GT' else x)\n",
        "image_df = image_df.dropna(axis=0)\n",
        "\n",
        "# ✅ Sample up to 200 images per class — skip classes with fewer images\n",
        "samples = []\n",
        "min_required = 200  # number of samples per class\n",
        "\n",
        "for category in image_df['Label'].unique():\n",
        "    category_slice = image_df.query(\"Label == @category\")\n",
        "    if len(category_slice) >= min_required:\n",
        "        samples.append(category_slice.sample(min_required, random_state=1))\n",
        "    else:\n",
        "        print(f\"Skipping class '{category}' — only {len(category_slice)} images available.\")\n",
        "\n",
        "# ✅ Check if we have valid data\n",
        "if not samples:\n",
        "    raise ValueError(\"❌ No categories had enough images to include in the dataset.\")\n",
        "\n",
        "# ✅ Final dataset\n",
        "image_df = pd.concat(samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T14:37:36.487615Z",
          "iopub.execute_input": "2025-05-09T14:37:36.487966Z",
          "iopub.status.idle": "2025-05-09T14:38:56.838221Z",
          "shell.execute_reply.started": "2025-05-09T14:37:36.487942Z",
          "shell.execute_reply": "2025-05-09T14:38:56.837348Z"
        },
        "id": "9h5arRiSYHdJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2, InceptionV3, EfficientNetB0\n",
        "\n",
        "# Assume image_df is already loaded and contains 'Filepath' and 'Label' columns\n",
        "train_df, val_df = train_test_split(image_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use MobileNetV2 preprocessing by default (works with all as input normalized between -1 and 1)\n",
        "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
        ")\n",
        "val_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
        ")\n",
        "\n",
        "train_images = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "val_images = val_generator.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "# Helper to create model on top of base model\n",
        "def create_model(base_model, num_classes):\n",
        "    base_model.trainable = False\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(base_model.output)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Store results\n",
        "results = []\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T14:39:17.098498Z",
          "iopub.execute_input": "2025-05-09T14:39:17.099333Z",
          "iopub.status.idle": "2025-05-09T14:39:17.14948Z",
          "shell.execute_reply.started": "2025-05-09T14:39:17.099292Z",
          "shell.execute_reply": "2025-05-09T14:39:17.148489Z"
        },
        "id": "bMxoHnznYHdK",
        "outputId": "82c6241e-1713-49ad-852d-9f9333ad0462"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Found 1440 validated image filenames belonging to 9 classes.\nFound 360 validated image filenames belonging to 9 classes.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pandas as pd\n",
        "\n",
        "# Make sure these are already defined earlier:\n",
        "# - create_model function\n",
        "# - train_images and val_images (ImageDataGenerators)\n",
        "# - results list\n",
        "\n",
        "# Step 1: Load VGG16 base model\n",
        "print(\"Training VGG16...\")\n",
        "vgg = VGG16(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "# Step 2: Create your model using your custom function\n",
        "model_vgg = create_model(vgg, num_classes=len(train_images.class_indices))\n",
        "\n",
        "# Step 3: Train the model\n",
        "history_vgg = model_vgg.fit(\n",
        "    train_images,\n",
        "    validation_data=val_images,\n",
        "    epochs=10,\n",
        "    callbacks=[\n",
        "        EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 4: Evaluate on validation data\n",
        "val_loss, val_acc = model_vgg.evaluate(val_images)\n",
        "\n",
        "# Step 5: Append the result\n",
        "results = []  # Ensure this list exists\n",
        "results.append({\"Model\": \"VGG16\", \"Val Accuracy\": val_acc})\n",
        "\n",
        "# Step 6: Save the trained model\n",
        "model_vgg.save(\"VGG16.h5\")\n",
        "print(\"VGG16 model saved as VGG16.h5\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T14:42:40.237954Z",
          "iopub.execute_input": "2025-05-09T14:42:40.238401Z",
          "iopub.status.idle": "2025-05-09T16:15:16.105872Z",
          "shell.execute_reply.started": "2025-05-09T14:42:40.238371Z",
          "shell.execute_reply": "2025-05-09T16:15:16.104632Z"
        },
        "id": "XAh1uy_SYHdM",
        "outputId": "b7f44355-ce91-458e-8d83-a07263f238c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training VGG16...\nEpoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 12s/step - accuracy: 0.2089 - loss: 2.1201 - val_accuracy: 0.5056 - val_loss: 1.6314\nEpoch 2/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 12s/step - accuracy: 0.6522 - loss: 1.4320 - val_accuracy: 0.7222 - val_loss: 1.0766\nEpoch 3/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 12s/step - accuracy: 0.7837 - loss: 0.8776 - val_accuracy: 0.8083 - val_loss: 0.6937\nEpoch 4/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 12s/step - accuracy: 0.8635 - loss: 0.5646 - val_accuracy: 0.8917 - val_loss: 0.4281\nEpoch 5/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 12s/step - accuracy: 0.9076 - loss: 0.3805 - val_accuracy: 0.9389 - val_loss: 0.3111\nEpoch 6/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 12s/step - accuracy: 0.9505 - loss: 0.2610 - val_accuracy: 0.9417 - val_loss: 0.2462\nEpoch 7/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 12s/step - accuracy: 0.9655 - loss: 0.2066 - val_accuracy: 0.9556 - val_loss: 0.2338\nEpoch 8/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m574s\u001b[0m 13s/step - accuracy: 0.9621 - loss: 0.1818 - val_accuracy: 0.9556 - val_loss: 0.1729\nEpoch 9/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 12s/step - accuracy: 0.9883 - loss: 0.1237 - val_accuracy: 0.9611 - val_loss: 0.1564\nEpoch 10/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m574s\u001b[0m 13s/step - accuracy: 0.9756 - loss: 0.1171 - val_accuracy: 0.9667 - val_loss: 0.1439\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 9s/step - accuracy: 0.9732 - loss: 0.1290\nVGG16 model saved as VGG16.h5\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Make sure these are already defined:\n",
        "# - create_model() function\n",
        "# - train_images and val_images (ImageDataGenerator)\n",
        "# - results list (should already exist or be initialized before this block)\n",
        "\n",
        "print(\"Training ResNet50...\")\n",
        "\n",
        "# Step 1: Load the ResNet50 base model\n",
        "resnet = ResNet50(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "# Step 2: Create your full model\n",
        "model_resnet = create_model(resnet, num_classes=len(train_images.class_indices))\n",
        "# etc.\n",
        "\n",
        "# Step 3: Train the model\n",
        "history_resnet = model_resnet.fit(\n",
        "    train_images,\n",
        "    validation_data=val_images,\n",
        "    epochs=10,\n",
        "    callbacks=[\n",
        "        EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 4: Evaluate the model\n",
        "val_loss, val_acc = model_resnet.evaluate(val_images)\n",
        "\n",
        "# Step 5: Append the result to results list\n",
        "results.append({\"Model\": \"ResNet50\", \"Val Accuracy\": val_acc})\n",
        "\n",
        "# Step 6: Save the model\n",
        "model_resnet.save(\"ResNet50.h5\")\n",
        "print(\"ResNet50 model saved as ResNet50.h5\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T16:15:16.107426Z",
          "iopub.execute_input": "2025-05-09T16:15:16.107708Z",
          "iopub.status.idle": "2025-05-09T16:47:00.788781Z",
          "shell.execute_reply.started": "2025-05-09T16:15:16.107688Z",
          "shell.execute_reply": "2025-05-09T16:47:00.787791Z"
        },
        "id": "QjTwRGw4YHdO",
        "outputId": "e83d8a56-3d2d-4185-9024-177e5ca69e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training ResNet50...\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 4s/step - accuracy: 0.1880 - loss: 2.1055 - val_accuracy: 0.4389 - val_loss: 1.7135\nEpoch 2/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 4s/step - accuracy: 0.3889 - loss: 1.7105 - val_accuracy: 0.4778 - val_loss: 1.4955\nEpoch 3/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 4s/step - accuracy: 0.4871 - loss: 1.4959 - val_accuracy: 0.5528 - val_loss: 1.3089\nEpoch 4/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 4s/step - accuracy: 0.5304 - loss: 1.3253 - val_accuracy: 0.5389 - val_loss: 1.2003\nEpoch 5/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 4s/step - accuracy: 0.5712 - loss: 1.2508 - val_accuracy: 0.5667 - val_loss: 1.1693\nEpoch 6/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 4s/step - accuracy: 0.6150 - loss: 1.1258 - val_accuracy: 0.5444 - val_loss: 1.2349\nEpoch 7/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 4s/step - accuracy: 0.6159 - loss: 1.1315 - val_accuracy: 0.6167 - val_loss: 1.0120\nEpoch 8/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 4s/step - accuracy: 0.6615 - loss: 0.9879 - val_accuracy: 0.6389 - val_loss: 1.0038\nEpoch 9/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 4s/step - accuracy: 0.6782 - loss: 0.9215 - val_accuracy: 0.6528 - val_loss: 0.9813\nEpoch 10/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 4s/step - accuracy: 0.7060 - loss: 0.8696 - val_accuracy: 0.7028 - val_loss: 0.8843\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 3s/step - accuracy: 0.7202 - loss: 0.8266\nResNet50 model saved as ResNet50.h5\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Assumes:\n",
        "# - `create_model()` is defined and returns a compiled model.\n",
        "# - `train_images` and `val_images` are preprocessed and ready ImageDataGenerators.\n",
        "# - `results` is a list already initialized to store results.\n",
        "\n",
        "print(\"Training MobileNetV2...\")\n",
        "\n",
        "# Step 1: Load the MobileNetV2 base model\n",
        "mobilenet = MobileNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "# Step 2: Create your full model\n",
        "model_mobilenet = create_model(mobilenet, len(train_images.class_indices))\n",
        "\n",
        "# Step 3: Train the model\n",
        "history_mobilenet = model_mobilenet.fit(\n",
        "    train_images,\n",
        "    validation_data=val_images,\n",
        "    epochs=10,\n",
        "    callbacks=[\n",
        "        EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 4: Evaluate the model\n",
        "val_loss, val_acc = model_mobilenet.evaluate(val_images)\n",
        "\n",
        "# Step 5: Append the result\n",
        "results.append({\"Model\": \"MobileNetV2\", \"Val Accuracy\": val_acc})\n",
        "\n",
        "# Step 6: Save the model\n",
        "model_mobilenet.save(\"MobileNetV2.h5\")\n",
        "print(\"MobileNetV2 model saved as MobileNetV2.h5\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T16:47:00.790119Z",
          "iopub.execute_input": "2025-05-09T16:47:00.790461Z",
          "iopub.status.idle": "2025-05-09T16:58:17.917103Z",
          "shell.execute_reply.started": "2025-05-09T16:47:00.790424Z",
          "shell.execute_reply": "2025-05-09T16:58:17.915976Z"
        },
        "id": "0iZoQ_g8YHdQ",
        "outputId": "518da70e-0415-4955-e053-0132cb284d31"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training MobileNetV2...\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - accuracy: 0.6099 - loss: 1.1726 - val_accuracy: 0.9556 - val_loss: 0.1253\nEpoch 2/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - accuracy: 0.9846 - loss: 0.0659 - val_accuracy: 0.9806 - val_loss: 0.0490\nEpoch 3/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - accuracy: 0.9971 - loss: 0.0232 - val_accuracy: 0.9917 - val_loss: 0.0367\nEpoch 4/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 0.9917 - val_loss: 0.0269\nEpoch 5/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.9861 - val_loss: 0.0275\nEpoch 6/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.9889 - val_loss: 0.0214\nEpoch 7/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9944 - val_loss: 0.0193\nEpoch 8/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9972 - val_loss: 0.0187\nEpoch 9/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9917 - val_loss: 0.0204\nEpoch 10/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 9.7324e-04 - val_accuracy: 0.9972 - val_loss: 0.0169\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 966ms/step - accuracy: 0.9991 - loss: 0.0159\nMobileNetV2 model saved as MobileNetV2.h5\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Assumes:\n",
        "# - create_model() is defined and returns a compiled model.\n",
        "# - train_images and val_images are prepared ImageDataGenerators.\n",
        "# - results is a list already initialized to store results.\n",
        "\n",
        "print(\"Training InceptionV3...\")\n",
        "\n",
        "# Step 1: Load the InceptionV3 base model\n",
        "inception = InceptionV3(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "# Step 2: Create your full model\n",
        "model_inception = create_model(inception, num_classes=len(train_images.class_indices))\n",
        "\n",
        "# Step 3: Train the model\n",
        "history_inception = model_inception.fit(\n",
        "    train_images,\n",
        "    validation_data=val_images,\n",
        "    epochs=10,\n",
        "    callbacks=[\n",
        "        EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Step 4: Evaluate the model\n",
        "val_loss, val_acc = model_inception.evaluate(val_images)\n",
        "\n",
        "# Step 5: Append the result\n",
        "results.append({\"Model\": \"InceptionV3\", \"Val Accuracy\": val_acc})\n",
        "\n",
        "# Step 6: Save the model\n",
        "model_inception.save(\"InceptionV3.h5\")\n",
        "print(\"InceptionV3 model saved as InceptionV3.h5\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T17:22:31.823771Z",
          "iopub.execute_input": "2025-05-09T17:22:31.825747Z",
          "iopub.status.idle": "2025-05-09T17:44:35.146959Z",
          "shell.execute_reply.started": "2025-05-09T17:22:31.825706Z",
          "shell.execute_reply": "2025-05-09T17:44:35.145878Z"
        },
        "id": "riBrsBtnYHdS",
        "outputId": "249c9b21-e018-4d57-b1f9-b8452ec81d9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training InceptionV3...\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 3s/step - accuracy: 0.5668 - loss: 1.3147 - val_accuracy: 0.8972 - val_loss: 0.2966\nEpoch 2/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 3s/step - accuracy: 0.9333 - loss: 0.2224 - val_accuracy: 0.9389 - val_loss: 0.1605\nEpoch 3/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 3s/step - accuracy: 0.9732 - loss: 0.0970 - val_accuracy: 0.9556 - val_loss: 0.1678\nEpoch 4/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 3s/step - accuracy: 0.9849 - loss: 0.0603 - val_accuracy: 0.9389 - val_loss: 0.1619\nEpoch 5/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 3s/step - accuracy: 0.9774 - loss: 0.0631 - val_accuracy: 0.9667 - val_loss: 0.1134\nEpoch 6/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 3s/step - accuracy: 0.9936 - loss: 0.0308 - val_accuracy: 0.9722 - val_loss: 0.0777\nEpoch 7/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0112 - val_accuracy: 0.9694 - val_loss: 0.0767\nEpoch 8/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.9778 - val_loss: 0.0718\nEpoch 9/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.9750 - val_loss: 0.0678\nEpoch 10/10\n\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.9722 - val_loss: 0.0729\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.9697 - loss: 0.0669\nInceptionV3 model saved as InceptionV3.h5\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T17:44:49.412406Z",
          "iopub.execute_input": "2025-05-09T17:44:49.412814Z",
          "iopub.status.idle": "2025-05-09T17:44:49.429288Z",
          "shell.execute_reply.started": "2025-05-09T17:44:49.412788Z",
          "shell.execute_reply": "2025-05-09T17:44:49.428176Z"
        },
        "id": "qYyYg_-MYHdU",
        "outputId": "25b4f545-9aaa-4924-b09d-9475088ffff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "         Model  Val Accuracy\n0        VGG16      0.966667\n1     ResNet50      0.702778\n2  MobileNetV2      0.997222\n3  InceptionV3      0.975000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Saving"
      ],
      "metadata": {
        "id": "LEA2hoaBYHdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you still have each trained model in memory:\n",
        "# Save each one manually like this:\n",
        "\n",
        "model_vgg.save(\"VGG16.h5\")\n",
        "model_resnet.save(\"ResNet50_v3.h5\")\n",
        "model_mobilenet.save(\"MobileNetV2.h5\")\n",
        "model_inception.save(\"InceptionV3.h5\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T17:48:28.110764Z",
          "iopub.execute_input": "2025-05-09T17:48:28.111628Z",
          "iopub.status.idle": "2025-05-09T17:48:29.899066Z",
          "shell.execute_reply.started": "2025-05-09T17:48:28.111601Z",
          "shell.execute_reply": "2025-05-09T17:48:29.898046Z"
        },
        "id": "yTP6HCvXYHdW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit"
      ],
      "metadata": {
        "id": "eQtQc0moYHdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T17:50:15.842946Z",
          "iopub.execute_input": "2025-05-09T17:50:15.843361Z",
          "iopub.status.idle": "2025-05-09T17:50:26.393387Z",
          "shell.execute_reply.started": "2025-05-09T17:50:15.843284Z",
          "shell.execute_reply": "2025-05-09T17:50:26.392056Z"
        },
        "id": "XCHEDeOVYHdY",
        "outputId": "ec9d304e-0d1f-44dc-eb6a-6182ef87ebad"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting streamlit\n  Downloading streamlit-1.45.0-py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\nRequirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.3)\nRequirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\nRequirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (19.0.1)\nRequirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.1)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.26.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nDownloading streamlit-1.45.0-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydeck, streamlit\nSuccessfully installed pydeck-0.9.1 streamlit-1.45.0\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "\n",
        "st.title(\"🐟 Fish Classifier App\")\n",
        "st.write(\"Upload a fish image and select a model to predict its class with confidence scores.\")\n",
        "\n",
        "# Available models and their preprocessing\n",
        "model_files = {\n",
        "    \"VGG16\": (\"VGG16.h5\", tf.keras.applications.vgg16.preprocess_input),\n",
        "    \"ResNet50 v3\": (\"ResNet50_v3.h5\", tf.keras.applications.resnet50.preprocess_input),\n",
        "    \"MobileNetV2\": (\"MobileNetV2.h5\", tf.keras.applications.mobilenet_v2.preprocess_input),\n",
        "    \"InceptionV3\": (\"InceptionV3.h5\", tf.keras.applications.inception_v3.preprocess_input),\n",
        "}\n",
        "\n",
        "model_choice = st.selectbox(\"Choose a model\", list(model_files.keys()))\n",
        "uploaded_image = st.file_uploader(\"Upload a fish image\", type=['jpg', 'jpeg', 'png'])\n",
        "\n",
        "# Load label mapping\n",
        "@st.cache_data\n",
        "def load_class_names():\n",
        "    try:\n",
        "        with open(\"class_indices.json\", \"r\") as f:\n",
        "            class_indices = json.load(f)\n",
        "        # Reverse map: index -> class name\n",
        "        return {v: k for k, v in class_indices.items()}\n",
        "    except:\n",
        "        return {i: f\"Class {i}\" for i in range(9)}  # fallback for 9 classes\n",
        "\n",
        "# Load model\n",
        "@st.cache_resource\n",
        "def load_model(model_path):\n",
        "    return tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Predict\n",
        "if uploaded_image is not None and model_choice:\n",
        "    model_path, preprocess_fn = model_files[model_choice]\n",
        "    model = load_model(model_path)\n",
        "    class_names = load_class_names()\n",
        "\n",
        "    image = Image.open(uploaded_image).convert('RGB')\n",
        "    image_resized = image.resize((224, 224))\n",
        "    image_array = tf.keras.preprocessing.image.img_to_array(image_resized)\n",
        "    image_array = preprocess_fn(image_array)\n",
        "    image_array = np.expand_dims(image_array, axis=0)\n",
        "\n",
        "    predictions = model.predict(image_array)[0]\n",
        "    predicted_index = np.argmax(predictions)\n",
        "    confidence = predictions[predicted_index]\n",
        "    predicted_label = class_names.get(predicted_index, f\"Class {predicted_index}\")\n",
        "\n",
        "    # Show results\n",
        "    st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "    st.subheader(f\"Predicted Class: 🐠 {predicted_label}\")\n",
        "    st.write(f\"Confidence: **{confidence * 100:.2f}%**\")\n",
        "\n",
        "    # Plot confidence scores\n",
        "    st.subheader(\"📊 Confidence Scores\")\n",
        "    fig, ax = plt.subplots()\n",
        "    bars = ax.bar(range(len(predictions)), predictions, tick_label=[class_names.get(i, f\"C{i}\") for i in range(len(predictions))])\n",
        "    ax.set_xlabel(\"Class\")\n",
        "    ax.set_ylabel(\"Confidence\")\n",
        "    ax.set_title(\"Confidence for Each Class\")\n",
        "\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2.0, yval + 0.01, f\"{yval:.2f}\", ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    st.pyplot(fig)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T17:51:40.66197Z",
          "iopub.execute_input": "2025-05-09T17:51:40.662409Z",
          "iopub.status.idle": "2025-05-09T17:51:40.70023Z",
          "shell.execute_reply.started": "2025-05-09T17:51:40.662384Z",
          "shell.execute_reply": "2025-05-09T17:51:40.699283Z"
        },
        "id": "EuCmOLtxYHda",
        "outputId": "b786ceaa-22ab-4db5-c64f-a2542de28468"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-05-09 17:51:40.673 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.676 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.678 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.679 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.680 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.681 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.682 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.683 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.684 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.685 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.686 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.687 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.689 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.692 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n2025-05-09 17:51:40.693 No runtime found, using MemoryCacheStorageManager\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom CNN"
      ],
      "metadata": {
        "id": "Ct7C9J1GYHde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "test_generator = ImageDataGenerator(rescale=1./255)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T17:47:38.308735Z",
          "iopub.execute_input": "2025-05-09T17:47:38.309139Z",
          "iopub.status.idle": "2025-05-09T17:47:38.337548Z",
          "shell.execute_reply.started": "2025-05-09T17:47:38.309116Z",
          "shell.execute_reply": "2025-05-09T17:47:38.336104Z"
        },
        "id": "9Xe1493dYHdf",
        "outputId": "cbe2cdae-d6eb-4774-d3ec-5d6b2f4bbd79"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_31/567704043.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'ImageDataGenerator' is not defined",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define Custom CNN model\n",
        "def create_custom_cnn(input_shape=(224, 224, 3), num_classes=9):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Conv Block 1\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Conv Block 2\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Conv Block 3\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Get number of classes dynamically from training data\n",
        "num_classes = train_images.num_classes\n",
        "\n",
        "# Create and train model\n",
        "model_cnn = create_custom_cnn(input_shape=(224, 224, 3), num_classes=num_classes)\n",
        "\n",
        "# Train\n",
        "history = model_cnn.fit(\n",
        "    train_images,\n",
        "    validation_data=val_images,\n",
        "    epochs=10,\n",
        "    callbacks=[callbacks.EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model_cnn.evaluate(test_images)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Save model\n",
        "model_cnn.save(\"Custom_CNN_FishClassifier.h5\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-09T17:47:45.421543Z",
          "iopub.execute_input": "2025-05-09T17:47:45.421953Z",
          "iopub.status.idle": "2025-05-09T17:47:45.457569Z",
          "shell.execute_reply.started": "2025-05-09T17:47:45.421924Z",
          "shell.execute_reply": "2025-05-09T17:47:45.45607Z"
        },
        "id": "zHzjaK98YHdg",
        "outputId": "7ee62946-c687-4e1f-f2da-024a868310f8"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_31/3964345141.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Get number of classes dynamically from training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Create and train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameIterator' object has no attribute 'num_classes'"
          ],
          "ename": "AttributeError",
          "evalue": "'DataFrameIterator' object has no attribute 'num_classes'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    }
  ]
}